import numpy as np
import pandas as pd
import sys
import mykmeanssp

def checkIfNum(num):
    try:
        num = float(num)
        intNum = int(num)
        return intNum == num 
    except ValueError:
        return False


def checkIfFloat(num):
    try:
        num = float(num)
        return True
    except ValueError:
        return False
    

def validate(args):
    if len(args) < 5 or len(args) > 6:
        print("An Error Has Occurred")
        sys.exit(1)
    
    try:
        K = args[1]
        if not checkIfNum(K) or int(float(K)) <= 1:
            raise ValueError
        K = int(float(K))
    except ValueError:
        print("Invalid number of clusters!")
        sys.exit(1)

    if len(args) == 6:
        try:
            iter = args[2]
            if not checkIfNum(iter) or int(float(iter)) <= 1 or int(float(iter)) >= 1000:
                raise ValueError
            iter = int(float(iter))
        except ValueError:
            print("Invalid maximum iteration!")
            sys.exit(1)
    else:
        iter = 300

    try:
        eps = args[-3]
        if not checkIfFloat(eps) or float(eps) < 0:
            raise ValueError
        eps = float(eps)
    except ValueError:
        print("Invalid epsilon!")
        sys.exit(1)

    file_name_1 = args[-2]
    file_name_2 = args[-1]

    return K, iter, eps, file_name_1, file_name_2


def loadData(file1, file2, K):
    df1 = pd.read_csv(file1, header=None)
    df2 = pd.read_csv(file2, header=None)

    # checking if the two files have the same number of observations (N)
    if df1.shape[0] != df2.shape[0]:
        print("An Error Has Occurred")
        sys.exit(1)

    if K >= df1.shape[0]:
        print("Invalid number of clusters!")
        sys.exit(1)

    merged_df = pd.merge(df1, df2, on=df1.columns[0])
    merged_df = merged_df.sort_values(by=merged_df.columns[0])
    data = merged_df.iloc[:, 1:].values
    keys = merged_df.iloc[:, 0].values
    return data, keys

def kmeans_pp(data, K):
    np.random.seed(1234)
    n = data.shape[0]
    centroids = np.zeros((K, data.shape[1]))
    centroids_idx = []
    first_centroid_idx = np.random.choice(n)
    centroids[0] = data[first_centroid_idx]
    centroids_idx.append(first_centroid_idx)
    
    for i in range(1, K):
        distances = np.linalg.norm(data[:, np.newaxis] - centroids[:i], axis=2)
        D = np.min(distances, axis=1)
        probabilities = D / np.sum(D)
        next_centroid_idx = np.random.choice(n, p=probabilities)
        centroids[i] = data[next_centroid_idx]
        centroids_idx.append(next_centroid_idx)
        
    return centroids, centroids_idx


K, iter, eps, file1, file2 = validate(sys.argv)
data, keys = loadData(file1, file2, K)
initial_centroids, centroids_idx = kmeans_pp(data, K)
initial_centroids = initial_centroids.tolist()
data = data.tolist()
final_centroids = mykmeanssp.fit(initial_centroids, data, K, iter, eps)

# printing
print(','.join(map(str, centroids_idx)))
for centroid in final_centroids:
    print(','.join(f'{c:.4f}' for c in centroid))

